{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from networkAlignmentAnalysis.models.registry import get_model\n",
    "from networkAlignmentAnalysis.datasets import get_dataset\n",
    "from networkAlignmentAnalysis.experiments.registry import get_experiment\n",
    "from networkAlignmentAnalysis import utils\n",
    "from networkAlignmentAnalysis import files\n",
    "from networkAlignmentAnalysis import train\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('using device: ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1.1. include additional AlignmentModel methods stored in extra class in base model\n",
    "\n",
    "# 4. Rewrite existing analysis pipelines\n",
    "# 5. SLURM!!!!\n",
    "\n",
    "# i don't like how by_stride is buried in the layers. Rewrite \n",
    "\n",
    "# Figure out why convolutional alignment measurement is slow...\n",
    "# still working on if it's possible to speed up measure_alignment for convolutional layers\n",
    "\n",
    "# Basic alignment_comparison Analyses (or maybe for alignment_stats):\n",
    "# - compare initial to final alignment...\n",
    "# - compare initial alignment to delta weight norm...\n",
    "# - observe alignment of delta weight\n",
    "# - compare alignment to outgoing delta weight norm!\n",
    "\n",
    "# Eigenfeature analyses:\n",
    "# done: - start by just looking at amplitude of activity on each eigenvector within each layer\n",
    "# - Determine contribution of each eigenfeature on performance with a eigenvector dropout experiment\n",
    "# - Measure beta_adversarial (figure out how adversarial examples map onto eigenvectors)\n",
    "\n",
    "# alignmentShaping.ipynb has an adversarial experiment worth looking at\n",
    "\n",
    "# Consider Valentin's idea about measuring an error threshold given signal and noise for a given level of alignment\n",
    "# e.g. plot a 2d heatmap comparing the noise amplitude and the average alignment\n",
    "# and then think about how to apply this to network design..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'CNN2P2'\n",
    "dataset_name = 'MNIST'\n",
    "by_stride = True\n",
    "\n",
    "net = get_model(model_name, build=True, dataset=dataset_name).to(DEVICE)\n",
    "dataset = get_dataset(dataset_name, build=True, transform_parameters=net, device=DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "# results = train.train([net], [optimizer], dataset, num_epochs=100, alignment=False)\n",
    "\n",
    "# beta, eigenvalue, eigenvector = net.measure_eigenfeatures(dataset.test_loader, by_stride=by_stride)\n",
    "# dropout_results = train.eigenvector_dropout([net], dataset, [eigenvalue], [eigenvector], train_set=False, by_stride=by_stride, by_layer=True)\n",
    "\n",
    "# plt.close('all')\n",
    "# plt.plot(results['accuracy'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# places that use 'unfold' or should use it\n",
    "# -- done -- AN.get_alignment_weights()\n",
    "# -- done --AN.forward_eigenvector.dropout() # --- doesn't use it but it should!!! --- \n",
    "# -- done -- AN.measure_eigenfeatures()\n",
    "# AN.measure_class_eigenfeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 ms ± 738 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "189 ms ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# ------------------ alignment functions ----------------------\n",
    "def alignment(input, weight, method='alignment'):\n",
    "    \"\"\"\n",
    "    measure alignment (proportion variance explained) between **input** and **weight**\n",
    "    \n",
    "    computes the rayleigh quotient between each weight vector in **weight** and the **input** fed \n",
    "    into **weight**. Typically, **input** is the output in Layer L-1 and **weight** is from Layer L\n",
    "\n",
    "    the output is normalized by the total variance in output of layer L-1 to measure the proportion \n",
    "    of variance of in **input** is explained by a projection onto node's weights in **weight**\n",
    "\n",
    "    args\n",
    "    ----\n",
    "        input: (batch, neurons) torch tensor \n",
    "            - represents input activity being fed in to network weight layer\n",
    "        weight: (num_out, num_in) torch tensor \n",
    "            - represents weights multiplied by input layer\n",
    "        method: string, default='alignment'\n",
    "            - which method to use to measure structure in **input** \n",
    "            - if 'alignment', uses covariance matrix of **input**\n",
    "            - if 'similarity', uses correlation matrix of **input**\n",
    "\n",
    "    returns\n",
    "    -------\n",
    "        alignment: (num_out, ) torch tensor\n",
    "            - proportion of variance explained by projection of **input** onto each **weight** vector\n",
    "    \"\"\"\n",
    "    assert method=='alignment' or method=='similarity', \"method must be set to either 'alignment' or 'similarity' (or None, default is alignment)\"\n",
    "    if method=='alignment':\n",
    "        cc = torch.cov(input.T)\n",
    "    elif method=='similarity':\n",
    "        cc = utils.smartcorr(input.T)\n",
    "    else: \n",
    "        raise ValueError(f\"did not recognize method ({method}), must be 'alignment' or 'similarity'\")\n",
    "    # Compute rayleigh quotient\n",
    "    rq = torch.sum(torch.matmul(weight, cc) * weight, axis=1) / torch.sum(weight * weight, axis=1)\n",
    "    # proportion of variance explained by a projection of the input onto each weight\n",
    "    prq = rq/torch.trace(cc)\n",
    "    return prq\n",
    "\n",
    "# B, D, S, C = 1024, 25, 784, 32\n",
    "B, D, S, C = 1024, 800, 196, 64\n",
    "input = torch.normal(0, 1, (B, D, S)).to(DEVICE)\n",
    "weight = torch.normal(0, 1, (C, D)).to(DEVICE)\n",
    "\n",
    "def get_align_usual(input, weight):\n",
    "    var_stride = torch.mean(torch.var(input, dim=1), dim=0)\n",
    "    align_stride = torch.stack([alignment(input[:, :, i], weight) for i in range(S)], dim=1)\n",
    "    return utils.weighted_average(align_stride, var_stride.view(1, -1), 1, ignore_nan=True)\n",
    "    \n",
    "def get_align_new(input, weight):\n",
    "    var_stride = torch.mean(torch.var(input, dim=1), dim=0)\n",
    "    cc = utils.batch_cov(input.transpose(0, 2))\n",
    "    rq = torch.sum(torch.matmul(weight, cc) * weight, axis=2) / torch.sum(weight * weight, axis=1)\n",
    "    prq = rq / torch.diagonal(cc, dim1=1, dim2=2).sum(1, keepdim=True)\n",
    "    return utils.weighted_average(prq, var_stride.view(-1, 1), 0, ignore_nan=True)\n",
    "\n",
    "au = get_align_usual(input, weight)\n",
    "an = get_align_new(input, weight)\n",
    "\n",
    "%timeit _ = get_align_usual(input, weight)\n",
    "%timeit _ = get_align_new(input, weight)\n",
    "\n",
    "print(torch.allclose(au, an))\n",
    "\n",
    "\n",
    "# convert batch_cov to allow for batch_corr too (and make it \"smart\")\n",
    "# integrate batched alignment into pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 25, 784])\n",
      "torch.Size([1024, 800, 196])\n",
      "torch.Size([1024, 3136])\n",
      "torch.Size([1024, 128])\n"
     ]
    }
   ],
   "source": [
    "images, labels = dataset.unwrap_batch(next(iter(dataset.test_loader)))\n",
    "\n",
    "inputs = net.get_layer_inputs(images, precomputed=False)\n",
    "processed = net._preprocess_inputs(inputs)\n",
    "processed = [p.cpu() for p in processed]\n",
    "\n",
    "for i in processed:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 25]) torch.Size([1024, 25, 25])\n"
     ]
    }
   ],
   "source": [
    "input = processed[0].clone()\n",
    "w, v = utils.smart_pca(input)\n",
    "print(w.shape, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# eigh will fail if condition number is too high (which can happen\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# in these strided input activities). If that's the case, we set the \u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# covariance to the identity so eigh will work, and set the variance\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# to 0 in that stride, so the weighted_average will ignore that stride.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, bc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bcov):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbc\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m     17\u001b[0m         bvar[ii] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# set variance to 0 to ignore this stride\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         bcov[ii] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(bcov\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# set to identity for simple eigvec decomp.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "centered = True\n",
    "input = processed[0].clone()\n",
    "\n",
    "bvar = torch.mean(torch.var(input, dim=1), dim=0) \n",
    "\n",
    "# measuring across each stride independently (for conv layers)\n",
    "bcov = utils.batch_cov(input.permute((2, 1, 0)), centered=centered)\n",
    "cond_inf = torch.zeros((len(bvar)))\n",
    "var_zero = torch.zeros((len(bvar)))\n",
    "\n",
    "# eigh will fail if condition number is too high (which can happen\n",
    "# in these strided input activities). If that's the case, we set the \n",
    "# covariance to the identity so eigh will work, and set the variance\n",
    "# to 0 in that stride, so the weighted_average will ignore that stride.\n",
    "for ii, bc in enumerate(bcov):\n",
    "    if torch.isinf(torch.linalg.cond(bc)):\n",
    "        bvar[ii] = 0 # set variance to 0 to ignore this stride\n",
    "        bcov[ii] = torch.eye(bcov.size(1)) # set to identity for simple eigvec decomp.\n",
    "        cond_inf[ii] = 1\n",
    "\n",
    "idx_inf = torch.where(cond_inf)[0]\n",
    "\n",
    "# measure eigenvalues and eigenvectors\n",
    "we, ve = utils.named_transpose([utils.eigendecomposition(bc, use_rank=True) for bc in bcov])\n",
    "# \n",
    "# stack \n",
    "we = torch.stack(we)\n",
    "ve = torch.stack(ve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 5, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_alignment_layers()[1].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "def batch_cov(input, centered=True, corr=False):\n",
    "    \"\"\"\n",
    "    Performs batched covariance on input data of shape (batch, dim, samples) or (dim, samples)\n",
    "\n",
    "    Where the resulting batch covariance matrix has shape (batch, dim, dim) or (dim, dim)\n",
    "    and bcov[i] = torch.cov(input[i]) if input.ndim==3\n",
    "\n",
    "    if centered=True (default=True) will subtract the means first\n",
    "    if corr=True (default=False), will divide by standard deviations to get correlation matrices\n",
    "          note that it's not really correlation if centered=False! \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def smartcorr(input):\n",
    "    \"\"\"\n",
    "    Performs torch corrcoef on the input data but sets each pair-wise correlation coefficent\n",
    "    to 0 where the activity has no variance (var=0) for a particular dimension (replaces nans with zeros)sss\n",
    "    \"\"\"\n",
    "    idx_zeros = torch.var(input, dim=1)==0\n",
    "    cc = torch.corrcoef(input)\n",
    "    cc[idx_zeros,:] = 0\n",
    "    cc[:,idx_zeros] = 0\n",
    "    return cc\n",
    "\n",
    "\n",
    "B, D, S = 1024, 7, 1000\n",
    "input = torch.normal(0, 1, (B, D, S))\n",
    "input[0, 0, :] = 0\n",
    "input[0, 3, :] = 0\n",
    "centered=True\n",
    "corr=True\n",
    "\n",
    "assert (input.ndim == 2) or (input.ndim == 3), \"input must be a 2D or 3D tensor\"\n",
    "# check if batch dimension was provided\n",
    "no_batch = input.ndim == 2 \n",
    "\n",
    "# add an empty batch dimension if not provided\n",
    "if no_batch: \n",
    "    input = input.unsqueeze(0) \n",
    "\n",
    "# measure number of samples of each input matrix\n",
    "S = input.size(2) \n",
    "\n",
    "# subtract mean if doing centered covariance\n",
    "if centered:\n",
    "    input = input - input.mean(dim=2, keepdim=True) \n",
    "\n",
    "if corr:\n",
    "    # measure standard deviation\n",
    "    input_dev = torch.std(input, dim=2)\n",
    "    dev_correction = input_dev.unsqueeze(2) * input_dev.unsqueeze(1)\n",
    "\n",
    "    # mask out any part of the correlation matrix with zeros where var=0\n",
    "    idx_zeros = input_dev==0\n",
    "    zero_mask = torch.logical_or(idx_zeros.unsqueeze(2), idx_zeros.unsqueeze(1))\n",
    "    \n",
    "\n",
    "# measure covariance of each input matrix\n",
    "bcov = torch.bmm(input, input.transpose(1, 2))\n",
    "\n",
    "if corr:\n",
    "    bcov /= dev_correction\n",
    "    bcov = bcov.masked_fill(zero_mask, 0)\n",
    "\n",
    "print(bcov.shape)\n",
    "\n",
    "# correct for number of samples\n",
    "bcov /= (S-1)\n",
    "\n",
    "# remove empty batch dimension if not provided\n",
    "if no_batch: \n",
    "    bcov = bcov.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate the changes in utils (e.g. smart_pca into _measure_layer_eigenfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkAlignmentAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
