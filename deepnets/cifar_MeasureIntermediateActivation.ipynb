{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5011699-37e6-43b4-9813-61645b38f5b4",
   "metadata": {},
   "source": [
    "## Note:\n",
    "This notebook was written before updates in integrationAnalysisTools, so probably won't work!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3455c199-2709-41b7-9a2b-f92e0c900153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import IPython\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import alignmentAnalysisTools as iat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2e471a-42b7-407a-b166-e0f309e769c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CPU = \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a206bf97-ac0e-4d1c-9417-161dbc152def",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = os.path.join('C:/', 'Users','andrew','Documents','machineLearning','datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8fa93d-7ca6-495a-b6af-6d8d79fcfa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "useNet = 'alexnet'\n",
    "useWeights = None # 'DEFAULT' or None\n",
    "if useNet=='resnet':\n",
    "    resnet = torchvision.models.resnet18(weights=useWeights)\n",
    "    resnet.to(DEVICE)\n",
    "    preprocess = transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                 ])\n",
    "elif useNet=='alexnet':\n",
    "    alexnet = torchvision.models.alexnet(weights=useWeights)\n",
    "    alexnet.to(DEVICE)\n",
    "    preprocess = transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                 ])\n",
    "elif useNet=='squeezenet':\n",
    "    squeezenet = torchvision.models.squeezenet1_0(weights=useWeights)\n",
    "    squeezenet.to(DEVICE)\n",
    "    preprocess = transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                 ])\n",
    "elif useNet=='efficientnet':\n",
    "    efficientnet = torchvision.models.efficientnet_b0(weights=useWeights)\n",
    "    efficientnet.to(DEVICE)\n",
    "    preprocess = transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                 ])\n",
    "else:\n",
    "    raise ValueError(\"useNet string not recognized\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6712120-506c-4c5c-8921-43486c8a5541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataSet = 'cifar10'\n",
    "if dataSet=='cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(root=dataPath, train=True, download=True, transform=preprocess)\n",
    "    testset = torchvision.datasets.CIFAR10(root=dataPath, train=False, download=True, transform=preprocess)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    numClasses = 10\n",
    "    \n",
    "elif dataSet=='cifar100':\n",
    "    trainset = torchvision.datasets.CIFAR100(root=dataPath, train=True, download=True, transform=preprocess)\n",
    "    testset = torchvision.datasets.CIFAR100(root=dataPath, train=False, download=True, transform=preprocess)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    numClasses = 100\n",
    "    \n",
    "elif dataSet=='imagenet':\n",
    "    trainset = torchvision.datasets.ImageNet(root=imagenetPath, train=True, download=True, transform=preprocess)\n",
    "    testset = torchvision.datasets.ImageNet(root=imagenetPath, train=False, download=True, transform=preprocess)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    numClasses = 1000\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Dataset not recognized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3689be41-a87e-44e5-8835-02dfb1abda7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d26a627c-5f41-4b5c-b851-226b96daa3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 5.826122999191284 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Define some hooks and return activations\n",
    "t = time.time()\n",
    "\n",
    "# Define Hook (will store activations of various levels in this dictionary)\n",
    "activation = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Now, for whatever network I requested, return activations for each batch (since we have dropout, do it with repetitions)\n",
    "numImages = 2**8\n",
    "\n",
    "# Do it\n",
    "if useNet=='resnet':\n",
    "    None\n",
    "    \n",
    "elif useNet=='alexnet':\n",
    "    im = []\n",
    "    feat2 = []\n",
    "    feat5 = []\n",
    "    feat7 = []\n",
    "    feat9 = []\n",
    "    input1 = []\n",
    "    linear1 = []\n",
    "    input4 = []\n",
    "    linear4 = []\n",
    "    alexnet.classifier[0].p=0 #Add this to maximize estimate of correlations \n",
    "    alexnet.classifier[3].p=0 #Add this to maximize estimate of correlations \n",
    "    alexnet.features[1].inplace=False #Need this set to False to allow full activations (before ReLU)\n",
    "    alexnet.classifier[2].inplace=False #Need this set to False to allow full activations (before ReLU)\n",
    "    alexnet.classifier[5].inplace=False #Need this set to False to allow full activations (before ReLU)\n",
    "    f2 = alexnet.features[2].register_forward_hook(getActivation('Feat2'))\n",
    "    f5 = alexnet.features[5].register_forward_hook(getActivation('Feat5'))\n",
    "    f7 = alexnet.features[7].register_forward_hook(getActivation('Feat7'))\n",
    "    f9 = alexnet.features[9].register_forward_hook(getActivation('Feat9'))\n",
    "    h1i = alexnet.classifier[0].register_forward_hook(getActivation('Dropout0')) # input to linear1\n",
    "    h1 = alexnet.classifier[1].register_forward_hook(getActivation('Linear1')) # output of linear1\n",
    "    h2i = alexnet.classifier[3].register_forward_hook(getActivation('Dropout3')) # input to linear4\n",
    "    h2 = alexnet.classifier[4].register_forward_hook(getActivation('Linear4')) # output of linear4\n",
    "    \n",
    "    imCount = 0\n",
    "    for batch in trainloader:\n",
    "        images, labels = batch\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        imCount += images.shape[0]\n",
    "\n",
    "        output = alexnet(images)\n",
    "        im.append(images.to(CPU))\n",
    "        feat2.append(activation['Feat2'].to(CPU))\n",
    "        feat5.append(activation['Feat5'].to(CPU))\n",
    "        feat7.append(activation['Feat7'].to(CPU))\n",
    "        feat9.append(activation['Feat9'].to(CPU))\n",
    "        input1.append(activation['Dropout0'].to(CPU))\n",
    "        linear1.append(activation['Linear1'].to(CPU))\n",
    "        input4.append(activation['Dropout3'].to(CPU))\n",
    "        linear4.append(activation['Linear4'].to(CPU))\n",
    "        if imCount >= numImages: \n",
    "            break\n",
    "            \n",
    "    # detach the hooks\n",
    "    f2.remove()\n",
    "    f5.remove()\n",
    "    f7.remove()\n",
    "    f9.remove()\n",
    "    h1i.remove()\n",
    "    h1.remove()\n",
    "    h2i.remove()\n",
    "    h2.remove()\n",
    "    \n",
    "    # Save summary variables\n",
    "    allImages = torch.cat(im)\n",
    "    allFeat2 = torch.cat(feat2)\n",
    "    allFeat5 = torch.cat(feat5)\n",
    "    allFeat7 = torch.cat(feat7)\n",
    "    allFeat9 = torch.cat(feat9)\n",
    "    allInput1 = torch.cat(input1)\n",
    "    allOutput1 = torch.cat(linear1)\n",
    "    allInput4 = torch.cat(input4)\n",
    "    allOutput4 = torch.cat(linear4)\n",
    "    weights1 = alexnet.classifier[1].weight.to(CPU).detach()\n",
    "    weights4 = alexnet.classifier[4].weight.to(CPU).detach()\n",
    "    print(f'Finished in {time.time() - t} seconds!')\n",
    "    \n",
    "    del im, feat2, feat5, feat7, feat9, input1, linear1, input4, linear4, activation\n",
    "    del images, labels\n",
    "    \n",
    "elif useNet=='squeezenet':\n",
    "    None\n",
    "\n",
    "elif useNet=='efficientnet':\n",
    "    None\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"Didn't recognize useNet name (should be one of the above listed in if/elifs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65507505-7809-4c5c-b690-5bad42bb720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 3, 224, 224])\n",
      "torch.Size([1024, 64, 27, 27])\n",
      "torch.Size([1024, 192, 13, 13])\n",
      "torch.Size([1024, 384, 13, 13])\n",
      "torch.Size([1024, 256, 13, 13])\n",
      "torch.Size([1024, 9216])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024, 4096])\n",
      "torch.Size([1024, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(allImages.shape)\n",
    "print(allFeat2.shape)\n",
    "print(allFeat5.shape)\n",
    "print(allFeat7.shape)\n",
    "print(allFeat9.shape)\n",
    "print(allInput1.shape)\n",
    "print(allOutput1.shape)\n",
    "print(allInput4.shape)\n",
    "print(allOutput4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28c63315-d02f-4709-9a00-db610324a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to pad: 0.7594590187072754 seconds.\n",
      "Time to preallocate integration matrix: 0.0 seconds.\n",
      "(4096, 363)\n",
      "torch.Size([64, 363])\n",
      "Time to prepare data for integration analysis: 0.014950037002563477 seconds.\n",
      "Time to measure integration once: 0.014949798583984375 seconds (size rq: torch.Size([64])).\n",
      "Estimate over all looks: 45.223140716552734.\n",
      "Time to measure integration across across all looks: 69.07755923271179 seconds (size intLayer: (64, 3025)).\n"
     ]
    }
   ],
   "source": [
    "inputActivity = allImages\n",
    "layer = alexnet.features[0]\n",
    "\n",
    "t = time.time()\n",
    "preprocess = transforms.Pad(layer.padding)\n",
    "pInput = np.array(preprocess(inputActivity))\n",
    "print(f\"Time to pad: {time.time() - t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "hMax, wMax = iat.getMaximumStrides(inputActivity.shape[2], inputActivity.shape[3], layer)\n",
    "numLooks = hMax * wMax\n",
    "numChannels = layer.out_channels\n",
    "intLayer = np.empty((numChannels,numLooks))\n",
    "print(f\"Time to preallocate integration matrix: {time.time() - t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "stride = (0,0)\n",
    "numImages = pInput.shape[0]\n",
    "wIdxStart, hIdxStart = np.meshgrid(np.arange(0,layer.kernel_size[0]), np.arange(0, layer.kernel_size[1]))\n",
    "numElements = pInput.shape[1] * layer.kernel_size[0] * layer.kernel_size[1]\n",
    "alignedInput = pInput[:,:,hIdxStart + stride[0]*layer.stride[0], wIdxStart + stride[1]*layer.stride[1]].reshape(numImages, numElements)\n",
    "alignedWeights = layer.weight.reshape(layer.out_channels, numElements).to('cpu').detach()\n",
    "print(alignedInput.shape)\n",
    "print(alignedWeights.shape)\n",
    "print(f\"Time to prepare data for integration analysis: {time.time() - t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "alignedInput = torch.tensor(alignedInput)\n",
    "idxMute = torch.where(torch.std(alignedInput,axis=0)==0)[0]\n",
    "b,n = alignedInput.shape\n",
    "m = alignedWeights.shape[1]\n",
    "cc = torch.corrcoef(alignedInput.T)\n",
    "cc[idxMute,:] = 0\n",
    "cc[:,idxMute] = 0\n",
    "rq = torch.sum(torch.matmul(alignedWeights,cc) * alignedWeights,axis=1) / torch.sum(alignedWeights*alignedWeights, axis=1)\n",
    "rq = -torch.log(rq/n)\n",
    "tOnce = time.time() - t\n",
    "tEstimate = tOnce * numLooks\n",
    "print(f\"Time to measure integration once: {tOnce} seconds (size rq: {rq.shape}).\")\n",
    "print(f\"Estimate over all looks: {tEstimate}.\")\n",
    "\n",
    "t = time.time()\n",
    "intLayer = iat.similarityConvLayer(allImages, alexnet.features[0])\n",
    "print(f\"Time to measure integration across across all looks: {time.time() - t} seconds (size intLayer: {intLayer.shape}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f015815-3eb7-427d-b4a7-c2f7f261d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intFeat0 finished: 17.113802433013916 seconds.\n",
      "intFeat3 finished: 22.20647621154785 seconds.\n",
      "intFeat6 finished: 5.3983283042907715 seconds.\n",
      "intFeat8 finished: 14.49635624885559 seconds.\n",
      "intFeat10 finished: 7.824768304824829 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Compute integration for the convolutional layers\n",
    "t = time.time()\n",
    "simFeat0 = iat.similarityConvLayer(allImages, alexnet.features[0])\n",
    "intFeat0 = iat.integration(simFeat0)\n",
    "print(f\"intFeat0 finished: {time.time()-t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "simFeat3 = iat.similarityConvLayer(allFeat2, alexnet.features[3])\n",
    "intFeat3 = iat.integration(simFeat3)\n",
    "print(f\"intFeat3 finished: {time.time()-t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "simFeat6 = iat.similarityConvLayer(allFeat5, alexnet.features[6])\n",
    "intFeat6 = iat.integration(simFeat6)\n",
    "print(f\"intFeat6 finished: {time.time()-t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "simFeat8 = iat.similarityConvLayer(allFeat7, alexnet.features[8])\n",
    "intFeat8 = iat.integration(simFeat8)\n",
    "print(f\"intFeat8 finished: {time.time()-t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "simFeat10 = iat.similarityConvLayer(allFeat9, alexnet.features[10])\n",
    "intFeat10 = iat.integration(simFeat10)\n",
    "print(f\"intFeat10 finished: {time.time()-t} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c7f8ba6-b540-4525-8ecd-45bc3f051a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andrew\\AppData\\Local\\Temp\\ipykernel_10736\\2090471768.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputActivity = torch.tensor(inputActivity)\n",
      "C:\\Users\\andrew\\AppData\\Local\\Temp\\ipykernel_10736\\2090471768.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights = torch.tensor(weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intClass1 finished: 1.4122743606567383 seconds.\n",
      "intClass4 finished: 0.2890329360961914 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Compute integration for the feedforward layers\n",
    "t = time.time()\n",
    "simClass1 = iat.similarity(allInput1, weights1)\n",
    "intClass1 = iat.integration(simClass1)\n",
    "print(f\"intClass1 finished: {time.time()-t} seconds.\")\n",
    "\n",
    "t = time.time()\n",
    "simClass4 = iat.similarity(allInput4, weights4)\n",
    "intClass4 = iat.integration(simClass4)\n",
    "print(f\"intClass4 finished: {time.time()-t} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8bd596a-13af-498b-9051-c17695619d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQ-Feat0: mean=6.371365611172658, std=1.1771545750873715.\n",
      "RQ-Feat3: mean=7.662070023816609, std=0.6305830852814601.\n",
      "RQ-Feat6: mean=7.674985692272407, std=0.4429299781592306.\n",
      "RQ-Feat8: mean=8.340740360815586, std=0.37125329957116604.\n",
      "RQ-Feat10: mean=7.95823271467885, std=0.3466117349915805.\n",
      "RQ-Class1: mean=9.214461326599121, std=0.25561967492103577.\n",
      "RQ-Class4: mean=8.512177467346191, std=0.22892655432224274.\n"
     ]
    }
   ],
   "source": [
    "print(f'RQ-Feat0: mean={np.mean(intFeat0)}, std={np.std(intFeat0)}.')\n",
    "print(f'RQ-Feat3: mean={np.mean(intFeat3)}, std={np.std(intFeat3)}.')\n",
    "print(f'RQ-Feat6: mean={np.mean(intFeat6)}, std={np.std(intFeat6)}.')\n",
    "print(f'RQ-Feat8: mean={np.mean(intFeat8)}, std={np.std(intFeat8)}.')\n",
    "print(f'RQ-Feat10: mean={np.mean(intFeat10)}, std={np.std(intFeat10)}.')\n",
    "print(f'RQ-Class1: mean={np.mean(intClass1)}, std={np.std(intClass1)}.')\n",
    "print(f'RQ-Class4: mean={np.mean(intClass4)}, std={np.std(intClass4)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c3b23-39ab-486b-a802-9bee06684b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c086ab1-61d5-456c-9955-4c2116837c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd48952-161d-4c6f-9106-6714a297fb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
