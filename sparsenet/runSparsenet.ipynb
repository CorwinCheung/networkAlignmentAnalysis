{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68db2d-43d2-4f91-beec-049798598c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "sys.path.append(\"../\") \n",
    "from sparsenet.ImageDataset import NatPatchDataset\n",
    "from sparsenet.atlSparseNet import SparseNet, argsSparseNet, initWeights\n",
    "from sparsenet.plotting import plot_rf\n",
    "from sparsenet import snExperiments as snExperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc2a72-16b8-4fd5-a5c4-7fcb76061ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg=argsSparseNet()\n",
    "print(arg.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4e22d-099b-478a-b130-5713ccc49647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to tensorboard\n",
    "# board = SummaryWriter(\"./runs/sparse-net\")    \n",
    "opts={'learning_rate':1e-2,'n_neuron':400,'epoch':50}\n",
    "arg = argsSparseNet(opts) # Defaults encoded in class\n",
    "# if use cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create net\n",
    "sparse_net = SparseNet(arg.n_neuron, arg.size, R_lr=arg.r_learning_rate, lmda=arg.reg, device=device)\n",
    "# load data\n",
    "batch_size = 500\n",
    "dataloader = DataLoader(NatPatchDataset(arg.batch_size, arg.size, arg.size, fpath='./data/IMAGES.mat'), batch_size=batch_size, shuffle=True)\n",
    "# train\n",
    "optim = torch.optim.SGD([{'params': sparse_net.U.weight, \"lr\": arg.learning_rate}])\n",
    "\n",
    "# track loss & measure integration\n",
    "frac2remove = 0.25\n",
    "num2remove = int(arg.n_neuron*frac2remove)\n",
    "trackLoss = torch.zeros(arg.epoch*len(dataloader))\n",
    "woutHiAlignLoss = torch.zeros(arg.epoch*len(dataloader))\n",
    "woutLoAlignLoss = torch.zeros(arg.epoch*len(dataloader))\n",
    "alignMean = torch.zeros((arg.n_neuron,arg.epoch*len(dataloader)))\n",
    "useForSmall = int(batch_size/4)\n",
    "batchIdx = 0\n",
    "for e in tqdm(range(arg.epoch)):\n",
    "    running_loss = 0\n",
    "    c = 0\n",
    "    for img_batch in dataloader: #tqdm(dataloader, desc='training', total=len(dataloader)):\n",
    "        img_batch = torch.flatten(img_batch,1).to(device)\n",
    "        # update\n",
    "        pred = sparse_net(img_batch)\n",
    "        loss = ((img_batch - pred) ** 2).sum()\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        # update U\n",
    "        optim.step()\n",
    "        # zero grad\n",
    "        sparse_net.zero_grad()\n",
    "        # norm\n",
    "        sparse_net.normalize_weights()\n",
    "        # save values\n",
    "        trackLoss[batchIdx]=loss.item()\n",
    "        alignMean[:,batchIdx]=sparse_net.measureAlignment(img_batch)\n",
    "        idxCurrentAlign = torch.argsort(alignMean[:,batchIdx])\n",
    "        idxWoutHi = idxCurrentAlign[:-num2remove]\n",
    "        idxWoutLo = idxCurrentAlign[num2remove:]\n",
    "        woutHiPred = sparse_net.forwardSubsetReuse(img_batch, idxWoutHi)\n",
    "        woutLoPred = sparse_net.forwardSubsetReuse(img_batch, idxWoutLo)\n",
    "        woutHiAlignLoss[batchIdx] = ((img_batch-woutHiPred)**2).sum().detach()\n",
    "        woutLoAlignLoss[batchIdx] = ((img_batch-woutLoPred)**2).sum().detach()\n",
    "        c += 1\n",
    "        batchIdx += 1\n",
    "    # And update occasionally\n",
    "    #if e%5==4:\n",
    "    #    fig = plot_rf(sparse_net.U.weight.T.reshape(arg.n_neuron, arg.size, arg.size).cpu().data.numpy(), arg.n_neuron, arg.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf842db-f6ec-4c3b-b9b9-22d83efb3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxInitAlign = torch.argsort(alignMean[:,0])\n",
    "num2look = 50\n",
    "highInit = idxInitAlign[-num2look:]\n",
    "lowInit = idxInitAlign[:num2look]\n",
    "fig,ax = plt.subplots(3,1,figsize=(5,12))\n",
    "ax[0].plot(range(arg.epoch*len(dataloader)), trackLoss, c='k', label='loss')\n",
    "ax[0].plot(range(arg.epoch*len(dataloader)), woutHiAlignLoss.detach(), c='r', label='woutHi')\n",
    "ax[0].plot(range(arg.epoch*len(dataloader)), woutLoAlignLoss.detach(), c='b', label='woutLo')\n",
    "\n",
    "# Plot Similarity\n",
    "ax[1].plot(range(arg.epoch*len(dataloader)), torch.mean(alignMean,axis=0), c='k', label='simFull')\n",
    "ax[1].plot(range(arg.epoch*len(dataloader)), torch.mean(alignMean[highInit,:],axis=0), c='r', label='alignInitialHigh')\n",
    "ax[1].plot(range(arg.epoch*len(dataloader)), torch.mean(alignMean[lowInit,:],axis=0), c='b', label='alignInitialLow')\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].legend(fontsize=7)\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_ylabel('Similarity')\n",
    "\n",
    "ax[2].scatter(trackLoss, torch.mean(alignMean,axis=0), c=range(len(trackLoss)), cmap='autumn', s=5)\n",
    "ax[2].set_xlabel('Loss')\n",
    "ax[2].set_ylabel('Similarity')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54b2c4-a9de-414c-aedd-3b8682e655fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = sparse_net.U.weight.T.reshape(arg.n_neuron, arg.size, arg.size).cpu().data.numpy()\n",
    "fig = plot_rf(rfs, arg.n_neuron, arg.size, showRFs=10**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe39246-fd01-452d-8e45-eeb43d6b87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure eigenvectors of images\n",
    "allImages = []\n",
    "for img_batch in dataloader:\n",
    "    allImages.append(torch.flatten(img_batch,1))\n",
    "allImages = torch.cat(allImages)\n",
    "imCov = torch.cov(allImages.T)\n",
    "imEval,imEvec = torch.linalg.eigh(imCov)\n",
    "imEval,idxEval = torch.sort(imEval, descending=True)\n",
    "imEvec = imEvec[:,idxEval]\n",
    "NEV = len(imEval)\n",
    "\n",
    "# Measure dot product of RFs on eigenvectors\n",
    "rfs = sparse_net.U.weight.clone().cpu().detach()\n",
    "rfs = rfs / torch.norm(rfs,dim=0) # make sure they are normed!\n",
    "beta = torch.abs(imEvec.T @ rfs)\n",
    "\n",
    "# Measure dot product of RFs on images\n",
    "rfActivation = allImages @ rfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097985f-0027-4135-909e-a7d2a27b503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9da4d3-e97b-4e31-afc9-1be043869acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make summary plots of betas\n",
    "fig,ax = plt.subplots(1,3,figsize=(12,4))\n",
    "\n",
    "# Plot betas\n",
    "mnBeta = torch.mean(beta, dim=1)\n",
    "seBeta = torch.std(beta, dim=1)/np.sqrt(beta.shape[1])\n",
    "# plt.plot(range(NEV), beta, c='lightgrey', linewidth=0.2, alpha=0.1)\n",
    "ax[0].plot(range(NEV), torch.mean(beta, dim=1), c='k', linewidth=1.5, label='coefficients')\n",
    "ax[0].fill_between(range(NEV), mnBeta+seBeta, mnBeta-seBeta, color='k', alpha=0.2)\n",
    "ax[0].plot(range(NEV), imEval, c='b', label='eigenvalues')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Eigenvector Number')\n",
    "ax[0].set_ylabel('Beta/Eigenvalue')\n",
    "ax[0].set_title('Loadings and Variance')\n",
    "\n",
    "# Plot sorted betas\n",
    "sortBeta = torch.sort(beta,dim=0,descending=True)[0]\n",
    "mnSortBeta = torch.mean(sortBeta,dim=1)\n",
    "seSortBeta = torch.std(sortBeta,dim=1)/np.sqrt(beta.shape[1])\n",
    "ax[1].plot(range(NEV), sortBeta, linewidth=0.3)\n",
    "# ax[1].plot(range(NEV), mnSortBeta, c='k', linewidth=0.5)\n",
    "# ax[1].fill_between(range(NEV), mnSortBeta+seSortBeta, mnSortBeta-seSortBeta, color='k', alpha=0.2)\n",
    "ax[1].plot(range(NEV), imEval, c='b', label='evals')\n",
    "ax[1].set_ylim(0)\n",
    "ax[1].set_xlabel('Eigenvector (sorted by betas)')\n",
    "ax[1].set_ylabel('Beta')\n",
    "ax[1].set_title('Loadings are well distributed')\n",
    "\n",
    "# Plot heatmap of all coefficients (sorted by COM)\n",
    "use2sort = 'eval'\n",
    "comVals = imEval if use2sort=='eval' else np.arange(NEV)\n",
    "comBeta = (comVals @ beta.numpy()) / torch.sum(beta * beta, dim=0)\n",
    "idxComBeta = torch.argsort(comBeta,descending=True)\n",
    "hm=ax[2].imshow(beta[:,idxComBeta].T,aspect='auto', cmap='hot', vmin=0, vmax=1)\n",
    "ax[2].set_xlabel('EigenVector')\n",
    "ax[2].set_ylabel('Neuron (aligned by com on eigenvectors)')\n",
    "ax[2].set_title('Coefficients on Each Eigenvector')\n",
    "plt.colorbar(hm, ax=ax[2], label='beta')\n",
    "\n",
    "# Make it pretty\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Compare entropy of betas to similarity\n",
    "normBeta = beta/torch.sum(beta,dim=0)\n",
    "entropy = -torch.nansum(normBeta * torch.log2(beta),dim=0)\n",
    "# plt.hist(entropy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69af017-6c0f-4a29-87bc-528a6e144064",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "mnBeta = torch.mean(beta, dim=1)\n",
    "seBeta = torch.std(beta, dim=1)/np.sqrt(beta.shape[1])\n",
    "# plt.plot(range(NEV), beta, c='lightgrey', linewidth=0.2, alpha=0.1)\n",
    "ax[0].plot(range(NEV), mnBeta, c='k', linewidth=1.5, label='coefficients')\n",
    "ax[0].fill_between(range(NEV), mnBeta+seBeta, mnBeta-seBeta, color='k', alpha=0.2)\n",
    "ax[0].plot(range(NEV), imEval, c='b', label='eigenvalues')\n",
    "\n",
    "scaledImEval = np.sqrt(imEval)\n",
    "ax[0].plot(range(NEV), scaledImEval, c='r', label='scaledEVAL')\n",
    "ax[0].plot(range(NEV), np.sqrt(mnBeta), c='g', label='scaledBeta')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Eigenvector Number')\n",
    "ax[0].set_ylabel('Beta/Eigenvalue')\n",
    "ax[0].set_title('Loadings and Variance')\n",
    "\n",
    "ax[1].scatter(imEval, mnBeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaeed46-d1e0-4e4e-804f-b1229334f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat0,p0 = normaltest(np.random.normal(0,1,rfActivation.shape),axis=0)\n",
    "stat,p = normaltest(rfActivation,axis=0)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "# Plot example activations\n",
    "ax[0].hist(rfActivation[:,1].numpy(), bins=100)\n",
    "ax[0].set_xlabel('Activations Ex Unit')\n",
    "ax[0].set_ylabel('Counts')\n",
    "\n",
    "# Compare distribution of stats with true normal\n",
    "ax[1].hist([stat0,stat], bins=50, label=['true normal','activations']);\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfe5d25-1bb6-46b4-acc1-e90dd368aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_net.U.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06693302-40fa-4745-9c48-d890250a2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs([rfCovariance.min(),rfCovariance.max()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c6f34-935e-46ee-832d-87b159a36b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tril(np.ones((5,5)),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc3541-d8c9-4a5a-b476-6987b1350def",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tril(np.ones(rfCovariance.shape),-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda791e0-140f-4914-99f6-a39577813d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "400**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807cf34-bf5b-4e7e-a94d-7f3f971467b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01ff2a-0ba1-46cf-af8c-6e323b74e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure covariance of RFs\n",
    "rfs = sparse_net.U.weight.T.cpu().detach().numpy()\n",
    "rfCovariance = np.cov(rfs)\n",
    "limMax = np.max(np.abs([rfCovariance.min(), rfCovariance.max()]))\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "imrfc = ax[0].imshow(rfCovariance,vmin=-limMax,vmax=limMax,cmap='seismic')\n",
    "plt.colorbar(imrfc, ax=ax[0])\n",
    "\n",
    "offDiagIdx = np.tril(np.ones(rfCovariance.shape),-1).astype(bool)\n",
    "offDiag = rfCovariance[offDiagIdx].reshape(-1,1)\n",
    "onDiag = np.diag(rfCovariance)\n",
    "\n",
    "bins = np.linspace(-limMax, limMax, 100)\n",
    "\n",
    "ax[1].hist(onDiag, bins, color='k', alpha=0.5, label='onDiagonal')\n",
    "ax[1].hist(offDiag, bins, color='b', alpha=0.5, label='offDiagonal')\n",
    "ax[1].legend(loc='upper left',fontsize=8)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c463e5-2592-482a-b232-08a83e63b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016f550-f3ac-4cb2-8b54-1d10d6f6fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = FastICA(n_components=40,max_iter=500, whiten=\"arbitrary-variance\", tol=15e-5)\n",
    "ica.fit(allImages)\n",
    "components = ica.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5ebc3-00fe-4510-8f2b-c80b5ff56993",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(components[0,:].reshape(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73bb27-3cb8-40a8-8581-c6fcae6f3dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a3d62-f591-4929-abff-384496231f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692c063-0460-4017-8d01-c3086bbfcf01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968afa9-a888-426e-ae93-23f4706d14e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab80af-688e-4804-8988-f702c78b9027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3a1f0-2f2b-42f8-9b01-a96dff4562b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892e553-d7e2-44ab-953f-2f307874865b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de5b89-e2c6-449f-b694-895da85828e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d8998-89e5-496d-95f4-9a0801b71970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad5a3c-5160-4f80-a975-749ef14299ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform many runs\n",
    "epochs = 100\n",
    "learningRate = (1e-2, 1e-3, 7e-4, 3e-4)\n",
    "numRuns = 10\n",
    "numLR = len(learningRate)\n",
    "\n",
    "doMultipleRuns = False\n",
    "if doMultipleRuns:\n",
    "    # Do Runs\n",
    "    for lr in range(numLR):\n",
    "        opts={'learning_rate':learningRate[lr],'n_neuron':400,'epoch':epochs}\n",
    "        for runIdx in range(numRuns):\n",
    "            print(f\"Learning Rate: {lr+1}/{numLR}, Run: {runIdx+1}/{numRuns}...\")\n",
    "            results = snExperiments.mainExperiment(opts)\n",
    "            # And Save\n",
    "            saveDir=Path('../data/firstPassSparsenet')\n",
    "            fileName=f'sparseNet{runIdx}_LR{lr}.pkl'\n",
    "            with open(saveDir / fileName, 'wb') as f:\n",
    "                pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab088509-f989-4f6d-8aac-fd1aba12dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple runs \n",
    "fileNames = glob(\"../data/firstPassSparsenet/*.pkl\")\n",
    "\n",
    "nets = []\n",
    "trackLoss = []\n",
    "noHiLoss = []\n",
    "noLoLoss = []\n",
    "alignment = []\n",
    "learningRate = []\n",
    "for file in fileNames:\n",
    "    with open(file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "        nets.append(results['sparse_net'])\n",
    "        trackLoss.append(results['trackLoss'])\n",
    "        alignment.append(results['alignMean'])\n",
    "        noHiLoss.append(results['woutHiAlignLoss'])\n",
    "        noLoLoss.append(results['woutLoAlignLoss'])\n",
    "        learningRate.append(results['arg'].learning_rate)\n",
    "learningRate = np.array(learningRate)\n",
    "lrVals = np.unique(learningRate)\n",
    "NLR = len(lrVals)\n",
    "\n",
    "# Just for the reminder...\n",
    "print(lrVals)\n",
    "print(f\"Keys available from results dictionary: {results.keys()}\")\n",
    "\n",
    "# Get better formatted summary statistics \n",
    "print(len(trackLoss))\n",
    "print(len(alignment))\n",
    "print(alignment[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff4d722-9d64-4196-b775-d48f8ed27b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Example (from last one loaded...)\n",
    "rfs = nets[0].U.weight.T.reshape(results['arg'].n_neuron, results['arg'].size, results['arg'].size).cpu().data.numpy()\n",
    "fig = plot_rf(rfs, results['arg'].n_neuron, results['arg'].size, showRFs=10**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6d934-2b19-48a9-bc9b-048ff1e99ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure eigenvectors of images\n",
    "allImages = []\n",
    "for img_batch in dataloader:\n",
    "    allImages.append(torch.flatten(img_batch,1))\n",
    "allImages = torch.cat(allImages)\n",
    "imCov = torch.cov(allImages.T)\n",
    "imEval,imEvec = torch.linalg.eigh(imCov)\n",
    "imEval,idxEval = torch.sort(imEval, descending=True)\n",
    "imEvec = imEvec[:,idxEval]\n",
    "NEV = len(imEval)\n",
    "\n",
    "# Measure dot product of RFs on eigenvectors and images\n",
    "beta = []\n",
    "rfActivation = []\n",
    "for sn in nets:\n",
    "    rfs = sn.U.weight.clone().detach().cpu()\n",
    "    rfs = rfs / torch.norm(rfs,dim=0) # make sure they are normed!\n",
    "    beta.append(torch.abs(imEvec.T @ rfs))\n",
    "    rfActivation.append(allImages @ rfs)\n",
    "\n",
    "# Make summary variables easier to access\n",
    "beta = torch.stack(beta)\n",
    "rfActivation = torch.stack(rfActivation)\n",
    "trackLoss = torch.stack(trackLoss)\n",
    "alignment = torch.stack(alignment)\n",
    "noHiLoss = torch.stack(noHiLoss)\n",
    "noLoLoss = torch.stack(noLoLoss)\n",
    "\n",
    "# For reference:\n",
    "# print(beta.shape)\n",
    "# print(rfActivation.shape)\n",
    "# print(trackLoss.shape)\n",
    "# print(alignment.shape)\n",
    "# print(noHiLoss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed06d9c-2209-4e84-bbe6-5c55d5d92db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02685bda-5811-4f94-b53c-ac758f8df7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cAlignMean = torch.mean(alignment[idxlr],dim=1)\n",
    "minAlign = torch.min(cAlignMean,dim=1)[0]\n",
    "maxAlign = torch.max(cAlignMean,dim=1)[0]\n",
    "align50 = torch.mean(torch.stack((minAlign,maxAlign)),dim=0)\n",
    "idxAlign50 = torch.argmax(torch.ones(cAlignMean.shape)*(cAlignMean.T<=align50).T,dim=1)\n",
    "print(minAlign.shape)\n",
    "print(maxAlign.shape)\n",
    "print(align50.shape)\n",
    "print(idxAlign50.shape)\n",
    "print(minAlign)\n",
    "print(maxAlign)\n",
    "print(align50)\n",
    "print(idxAlign50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895e74d-9b6b-4cd1-98cc-4d07b14639a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelSize=20\n",
    "missingPoints = (kernelSize-1)\n",
    "kernel = np.ones(kernelSize)/kernelSize\n",
    "smooth = lambda x: np.convolve(x,kernel,mode='valid')\n",
    "\n",
    "numEpoch = trackLoss.shape[1]\n",
    "\n",
    "fig,ax = plt.subplots(1,4,figsize=(12,3))\n",
    "cmap = cm.get_cmap('Set1', NLR).colors\n",
    "\n",
    "# Plot loss trajectories\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    mnLoss = torch.mean(trackLoss[idxlr,:],dim=0)\n",
    "    seLoss = torch.std(trackLoss[idxlr,:],dim=0)/np.sqrt(np.sum(idxlr))\n",
    "    mnHiLoss = torch.mean(noHiLoss[idxlr,:],dim=0)\n",
    "    seHiLoss = torch.std(noHiLoss[idxlr,:],dim=0)/np.sqrt(np.sum(idxlr))\n",
    "    mnLoLoss = torch.mean(noLoLoss[idxlr,:],dim=0)\n",
    "    seLoLoss = torch.std(noLoLoss[idxlr,:],dim=0)/np.sqrt(np.sum(idxlr))\n",
    "    ax[0].plot(range(numEpoch), mnLoss, c=cmap[lr], label=f'lr#{lr}', linewidth=0.5)\n",
    "    ax[0].fill_between(range(numEpoch), mnLoss+seLoss, mnLoss-seLoss, color=cmap[lr], alpha=0.2)\n",
    "    # ax[0].plot(range(numEpoch), mnHiLoss, c=cmap[lr], linewidth=0.5)\n",
    "    # ax[0].fill_between(range(numEpoch), mnHiLoss+seHiLoss, mnHiLoss-seHiLoss, color=cmap[lr], alpha=0.2)\n",
    "    # ax[0].plot(range(numEpoch), mnLoLoss, c=cmap[lr], linewidth=0.5)\n",
    "    # ax[0].fill_between(range(numEpoch), mnLoLoss+seLoLoss, mnLoLoss-seLoLoss, color=cmap[lr], alpha=0.2)\n",
    "    if lr==0:\n",
    "        endLoss = torch.mean(mnLoss[-10:])\n",
    "        endHiLoss = torch.mean(mnHiLoss[-10:])\n",
    "        endLoLoss = torch.mean(mnLoLoss[-10:])\n",
    "        ax[0].annotate('Total Loss', (numEpoch, endLoss*1.07), ha='right', fontsize=8)\n",
    "        # ax[0].annotate('w/out most aligned', (numEpoch, endHiLoss*1.07), ha='right', fontsize=8)\n",
    "        # ax[0].annotate('w/out least aligned', (numEpoch, endLoLoss*1.07), ha='right', fontsize=8)\n",
    "ax[0].legend(fontsize=7)\n",
    "ax[0].set_xlabel('Training Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "\n",
    "# Plot alignment trajectories\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    seCorrection = np.sqrt(np.sum(idxlr)) * alignment.shape[1]\n",
    "    mnAlignment = torch.mean(alignment[idxlr,:],dim=(0,1))\n",
    "    seAlignment = torch.std(alignment[idxlr,:],dim=(0,1))/seCorrection\n",
    "    ax[1].plot(range(numEpoch), mnAlignment, c=cmap[lr], label=f'lr#{lr}', linewidth=0.5)\n",
    "    ax[1].fill_between(range(numEpoch), mnAlignment+seAlignment, mnAlignment-seAlignment, color=cmap[lr], alpha=0.2)\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('Training Epoch')\n",
    "ax[1].set_ylabel('Alignment')\n",
    "\n",
    "# Compare 50% points of loss and alignment\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    minLoss = torch.min(trackLoss[idxlr],dim=1)[0]\n",
    "    maxLoss = torch.max(trackLoss[idxlr],dim=1)[0]\n",
    "    loss50 = torch.mean(torch.stack((minLoss, maxLoss)),dim=0)\n",
    "    idxLoss50 = torch.argmax(torch.ones(trackLoss[idxlr].shape)*(trackLoss[idxlr].T<=loss50).T,dim=1)\n",
    "    cAlignMean = torch.mean(alignment[idxlr],dim=1)\n",
    "    minAlign = torch.min(cAlignMean,dim=1)[0]\n",
    "    maxAlign = torch.max(cAlignMean,dim=1)[0]\n",
    "    align50 = torch.mean(torch.stack((minAlign,maxAlign)),dim=0)\n",
    "    idxAlign50 = torch.argmax(torch.ones(cAlignMean.shape)*(cAlignMean.T>=align50).T,dim=1)\n",
    "    ax[2].scatter(idxLoss50, idxAlign50, color=cmap[lr], label=f'lr#{lr}', s=4)\n",
    "ax[2].set_xlabel('Epochs until loss<50%')\n",
    "ax[2].set_ylabel('Epochs until mean alignment<50%')\n",
    "ax[2].legend()\n",
    "\n",
    "# Plot comparison of loss and similarity\n",
    "minInit = torch.min(alignment[:,:,0])\n",
    "maxInit = torch.max(alignment[:,:,0])\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    initAlign = alignment[idxlr,:,0]\n",
    "    finalAlign = alignment[idxlr,:,-1]\n",
    "    ax[3].scatter(initAlign,finalAlign,color=cmap[lr], s=4, alpha=0.25)\n",
    "    for ia,fa in zip(initAlign, finalAlign):\n",
    "        dmat = torch.stack((torch.ones(len(ia)),ia))\n",
    "        prms = torch.linalg.pinv(dmat.T) @ fa.T\n",
    "        ax[3].plot([minInit,maxInit],prms[0]+prms[1]*np.array([minInit,maxInit]),c=cmap[lr],alpha=0.25,linewidth=0.5)\n",
    "    dmat = torch.stack((torch.ones(torch.numel(initAlign)),torch.flatten(initAlign)))\n",
    "    prms = torch.linalg.pinv(dmat.T) @ torch.flatten(finalAlign)\n",
    "    ax[3].plot([minInit,maxInit],prms[0]+prms[1]*np.array([minInit,maxInit]),c=cmap[lr],alpha=1,linewidth=2.5)\n",
    "ax[3].set_xlim(minInit, maxInit)\n",
    "ax[3].set_xlabel('Initial Alignment (each node)')\n",
    "ax[3].set_ylabel('Final Alignment (each node)')\n",
    "\n",
    "# Make it pretty\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7206cc5-c5a0-4f40-b95d-d2b8a8ecf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5772eec-38ed-446d-a2f1-5d27d8febb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortBeta = torch.sort(beta[idxlr],dim=1,descending=True)[0]\n",
    "sortBeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093f666-7b5b-4a08-bf1d-581de45b0a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make summary plots of betas\n",
    "fig,ax = plt.subplots(1,3,figsize=(12,3.5))\n",
    "\n",
    "# Plot betas\n",
    "ax[0].plot(range(NEV), imEval, c='k', linewidth=1.5, label='eigenvalues')\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    seCorrection = np.sqrt(np.sum(idxlr) * beta.shape[2])\n",
    "    mnBeta = torch.mean(beta[idxlr],dim=(0,2))\n",
    "    seBeta = torch.std(beta[idxlr],dim=(0,2))/seCorrection\n",
    "    ax[0].plot(range(NEV), mnBeta, c=cmap[lr], label=f'beta lr#{lr}', linewidth=0.5)\n",
    "    ax[0].fill_between(range(NEV), mnBeta+seBeta, mnBeta-seBeta, color=cmap[lr], alpha=0.2)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Eigenvector Number')\n",
    "ax[0].set_ylabel('Beta/Eigenvalue')\n",
    "ax[0].set_title('Loadings on Eigenvectors')\n",
    "\n",
    "# Plot histogram of maximum beta (and second maximum beta) --- or I could do a drop off line plot? Sort each column then plot the mean / error...\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    seCorrection = np.sqrt(np.sum(idxlr)*beta.shape[2])\n",
    "    sortBeta = torch.sort(beta[idxlr],dim=1,descending=True)[0]\n",
    "    mnSortBeta = torch.mean(sortBeta,dim=(0,2))\n",
    "    seSortBeta = torch.std(sortBeta,dim=(0,2))/seCorrection\n",
    "    ax[1].plot(range(NEV), mnSortBeta, c=cmap[lr], linewidth=0.5, label=f'beta lr#{lr}')\n",
    "    ax[1].fill_between(range(NEV), mnSortBeta+seSortBeta, mnSortBeta-seSortBeta, color=cmap[lr], alpha=0.2)\n",
    "ax[1].set_ylim(0)\n",
    "ax[1].set_xlabel('Eigenvector (sorted by betas)')\n",
    "ax[1].set_ylabel('Beta')\n",
    "ax[1].set_title('Loadings are well distributed')\n",
    "\n",
    "# Plot heatmap of all coefficients (sorted by COM)\n",
    "exBeta = beta[0]\n",
    "use2sort = 'eval'\n",
    "useEvalScale = False\n",
    "scale = imEval/torch.max(imEval) if useEvalScale else torch.ones(imEval.shape)\n",
    "comVals = imEval if use2sort=='eval' else np.arange(NEV)\n",
    "comBeta = (comVals @ exBeta.numpy()) / torch.sum(exBeta * exBeta, dim=0)\n",
    "idxComBeta = torch.argsort(comBeta,descending=True)\n",
    "hm=ax[2].imshow(scale*exBeta[:,idxComBeta].T,aspect='auto', cmap='hot', vmin=0, vmax=1)\n",
    "ax[2].set_xlabel('EigenVector')\n",
    "ax[2].set_ylabel('Neuron (aligned by com)')\n",
    "ax[2].set_title('Example Coefficients')\n",
    "plt.colorbar(hm, ax=ax[2], label='beta')\n",
    "\n",
    "# # Make it pretty\n",
    "plt.tight_layout()\n",
    "\n",
    "# # Compare entropy of betas to similarity\n",
    "# normBeta = torch.permute(torch.permute(beta, (1,0,2))/torch.sum(beta,dim=1), (1,0,2))\n",
    "# entropy = -torch.nansum(normBeta * torch.log2(beta),dim=1)\n",
    "# # plt.hist(entropy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef307252-fca8-4fd1-bb19-63b36baecc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(12,3.5))\n",
    "\n",
    "# Plot betas\n",
    "ax[0].plot(range(NEV), imEval, c='k', linewidth=1.5, label='eigenvalues')\n",
    "for lr in range(NLR):\n",
    "    idxlr = learningRate==lrVals[lr]\n",
    "    seCorrection = np.sqrt(np.sum(idxlr) * beta.shape[2])\n",
    "    mnBeta = torch.mean(beta[idxlr],dim=(0,2))\n",
    "    seBeta = torch.std(beta[idxlr],dim=(0,2))/seCorrection\n",
    "    ax[0].plot(range(NEV), mnBeta, c=cmap[lr], label=f'beta lr#{lr}', linewidth=0.5)\n",
    "    ax[0].fill_between(range(NEV), mnBeta+seBeta, mnBeta-seBeta, color=cmap[lr], alpha=0.2)\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Eigenvector Number')\n",
    "ax[0].set_ylabel('Beta/Eigenvalue')\n",
    "ax[0].set_title('Loadings on Eigenvectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4604a1-d0ae-4193-8750-5cbf5a7790df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.average(ev1,weights=ev1)\n",
    "%timeit ev1@ev1/np.sum(ev1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bf026-3daa-4ac2-8a74-f43985f440d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = np.arange(1,100)\n",
    "pk = 0.3\n",
    "\n",
    "ev1 = np.exp(-xr/20)\n",
    "ev2 = 1.1**(-xr)\n",
    "ev3 = 1/(xr**2)\n",
    "ev1 = ev1/np.max(ev1)*pk\n",
    "ev2 = ev2/np.max(ev2)*pk\n",
    "ev3 = ev3/np.max(ev3)*pk\n",
    "plt.plot(xr,ev1,c='k')\n",
    "plt.plot(xr,ev2,c='r')\n",
    "plt.plot(xr,ev3,c='b')\n",
    "\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "print(f\"wAvgEv1:{np.average(ev1,weights=ev1):.3f}, wAvgEv2:{np.average(ev2,weights=ev2):.3f}, wAvgEv3:{np.average(ev3,weights=ev3):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dd503-e98d-4d91-aa27-829cc4092959",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat0,p0 = normaltest(np.random.normal(0,1,rfActivation.shape),axis=0)\n",
    "stat,p = normaltest(rfActivation,axis=0)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "# Plot example activations\n",
    "ax[0].hist(rfActivation[:,1].numpy(), bins=100)\n",
    "ax[0].set_xlabel('Activations Ex Unit')\n",
    "ax[0].set_ylabel('Counts')\n",
    "\n",
    "# Compare distribution of stats with true normal\n",
    "ax[1].hist([stat0,stat], bins=50, label=['true normal','activations']);\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e5c6f-0754-4016-94eb-b4febefa4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "imEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262ea1e-1631-4627-a28d-6bc1283f49ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831c1a4-8319-4f15-a2b5-6ffb26facd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35955c49-1918-4df4-8d55-329ee23b63f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8aa54a-1ff3-4da1-ab71-6ef234e182a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2f989-881d-47c7-bfad-8cc7c3dd74e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07cd4b-442b-402d-8f1f-983b68595f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7059a4d-9281-46a9-9691-e1e4cf914b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51373ade-cc31-4a2f-b3ff-bb1db6a2dc3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e6e01-4753-4d61-ab51-3e2b11056ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737459c4-8ba7-458d-b171-78cb7d1e9d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aab22b-7ca4-4526-8f44-3f36e1bc7b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783beca-0f67-49c7-9d82-d7e49e683fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c6c47-7e4b-4ca5-b98b-8c6fb662288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of each epoch loop it ran this:\n",
    "#board.add_scalar('Loss', running_loss / c, e * len(dataloader) + c)\n",
    "    #if e % 5 == 4:\n",
    "        # plotting\n",
    "        #fig = plot_rf(sparse_net.U.weight.T.reshape(arg.n_neuron, arg.size, arg.size).cpu().data.numpy(), arg.n_neuron, arg.size)\n",
    "        #board.add_figure('RF', fig, global_step=e * len(dataloader) + c)\n",
    "    #if e % 10 == 9:\n",
    "        # save checkpoint\n",
    "        #torch.save(sparse_net, f\"../../trained_models/ckpt-{e+1}.pth\")\n",
    "\n",
    "# And ran this at the end end\n",
    "#torch.save(sparse_net, f\"../../trained_models/ckpt-{e+1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
